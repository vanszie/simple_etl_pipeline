# ğŸ“ Simple ETL Pipeline - Weekly Course Attendance Report

This project is an ETL pipeline built in Python using pandas. It processes university course data to generate a weekly course attendance report (in %) by semester and week.

---

## ğŸ“Š Report Output

The final report is saved as a CSV file:

`data/mart/weekly_course_attendance.csv`

### Sample Output Format:

| SEMESTER_ID | WEEK_ID | COURSE_NAME        | ATTENDANCE_PCT |
|-------------|---------|--------------------|----------------|
| 1           | 1       | Calculus I         | 94.12          |
| 1           | 2       | Introduction to AI | 88.89          |

---

## ğŸ“ Folder Structure

. â”œâ”€â”€ Dockerfile 
  â”œâ”€â”€ etl_pipeline.py 
  â”œâ”€â”€ README.md 
  â””â”€â”€ data/ 
    â”œâ”€â”€ source/ â”‚ 
        â”œâ”€â”€ course.csv â”‚ 
        â”œâ”€â”€ schedule.csv â”‚ 
        â”œâ”€â”€ enrollment.csv â”‚ 
        â””â”€â”€ course_attendance.csv 
    â”œâ”€â”€ staging/ # Generated during ETL run
    â”œâ”€â”€ dwh/ # Generated during ETL run 
    â””â”€â”€ mart/ # Contains final CSV output

## ğŸ³ Running with Docker

### 1. Build Docker Image

docker build -t etl-pipeline .

### 2. Run Docker Container
Mount the local data/ directory so that you can access the output:

docker run --rm -v "$PWD/data":/app/data etl-pipeline

## âš™ï¸ What It Does

### Source Layer
Extract all csv file into pandas datafreame

### Staging Layer
Renames columns, parses datetimes, and adds a file_creation_datetime and load_datetime columns to all staging CSVs.

### dwh Layer
Joins the course, schedule, enrollment, and attendance data. 
save join_data to file fact_course_attendance_detail.csv

### Mart Layer
Calculates & summarize weekly attendance per course by semester.
Outputs a clean CSV report: weekly_course_attendance.csv

## â¬‡ï¸ Requirements (for local run)
If not using Docker:

pip install pandas
python etl_pipeline.py